\section{Experimental results}
In this section we present preliminary experimental results for the simplicial neural network. The datasets we analyze have been extracted from the Semantic Scholar datasets. The data consists of XXX papers together with their authors and number of citations. We retain paper with more than $5$ citations and at most $10$ authors.An important step in preprocessing many kinds of input data in TDA is constructing a simplicial complex. Our work focus on \emph{co-authorship complexes} (or \emph{collaboration complexes})~\cite{patania2017}, simplicial complexes where a paper with $k$ authors is represented by a $(k-1)$-simplex. We constructed different co-authorship complexes by considering sub-samplings from the papers set of the Semantic Scholar dataset. The sub-samplings were obtained by performing random walks (of length 80) on the nodes of the graph which vertices corresponds to the papers and edges connect papers sharing at least one author. The co-authorship complexes obtained from each sub-sampling  have corresponding $k$ cochains given by the number of shared citations of the $k$-collaborations (see Figure~\ref{fig:data2complex}).We evaluate the performance of SNNs on the task of predicting missing input data. As in a typical pipeline for this task, in our approach missing data is first replaced by some values. Specifically, given a fixed co-authorship complex missing data is introduced 
at random on the training cochains at $4$ levels: $10\%,  20\%,  30\%$, and $50\% $.   In our case the training input is given by the citations on the co-authorship complex where the random missing data is substituted by the median of the known data. We trained a SNN composed by $3$-layers with $30$ convolutional filters of degree $5$. We used the $L_1$ norm as reconstruction loss over the known elements an the Adam optimizer with learning rate of $1\times 10^{-3}$. The SNN was trained for $1000$ iterations. We then test the performance of the network on its accuracy in predicting the missing data. A predicted citation is considered correct if the predicted value differs of at most $1$ from the actual number of citations. The prediction error is the absolute value of the difference between the predicted citation and the actual value of the citation. Figure~\ref{fig:accuracy-error} (a) shows the accuracy of the SNN in prediction missing citations on CC1 (Co-authorship Complex 1, Table~\ref{table:Simplices-coauthor}). The distribution of the prediction error is shown in Figure~\ref{fig:accuracy-error}. 
\begin{table}[htbp]
  \caption{%
  Number of simplices
  }
  \label{table:Simplices-coauthor}
  \centering
  \scriptsize{
  \begin{tabular}{llllllllllll}
    \cmidrule(r){1-12}
    Dimension   & 0     & 1  & 2     & 3 & 4     & 5 & 6    & 7 & 8   & 9 & 10\\
    \midrule
    CC1 & 352  & 1474  & 3285  & 5019  & 5559  & 4547  & 2732  & 1175  & 343 & 61 & 5\\
    CC2 & 1126 & 5059 & 11840 & 18822 & 21472 & 17896  & 10847 & 4673 & 1357 & 238 & 19\\ 
    \bottomrule
  \end{tabular}}
\end{table}
%\begin{figure}[htbp]
%  \centering
%\includegraphics[scale=0.35]{./figures/distribution_cohain_150250.png}
% \caption{Distribution of the citation in CC1 } \label{fig:accuracy}
%\end{figure}
\begin{figure}[tb]
\centering
 \begin{subfigure}[t]{-0.8\textwidth}
 \vspace{-4cm}
    \text{(a)}
  \end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\centering
   \includegraphics[scale=0.35]{./figures/accuracy_network1.png}
 %\caption{Accuracy of SNN in predicting missing citations } \label{fig:accuracy}
\end{subfigure}
 \begin{subfigure}[t]{0.8\textwidth}
    \text{(b)}
  \end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\centering
\vspace{-0.5cm}
   \includegraphics[scale=0.36]{./figures/Error_dist_start150250_seed6666_notsee40.png}
 % \caption{Distribution of the prediction's error} \label{fig:error}
\end{subfigure}
\caption{(a) Accuracy of SNN in predicting missing citations. (b)Distribution of the prediction's error}
\label{fig:accuracy-error}
\end{figure}
Transfer learning was used as a second assessment for our network. In particular, we test how accurately a SNN pretrained on a co-authorship complex can predict citations on a different complex. Figure ~\ref{fig:transfer-learning} shows the accuracy on predicting missing citations on CC1 using the above architecture of SNN trained on CC2 (Co-authorship Complex 2, see Table~\ref{table:Simplices-coauthor}).

\stefania{Tell conclusion results, say something about baseline, say something about dimension 2.}
\stefania{Say length random walks}


\begin{figure}[htbp]
  \centering
\includegraphics[scale=0.35]{./figures/accuracy_network1_pretrained.png}
  \caption{Accuracy in predicting missing citations with a pretrained SNN } \label{fig:transfer-learning}
\end{figure}

