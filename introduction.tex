\section{Introduction}



Graph-based convolutional neural networks (GNNs) have recently become popular techniques in machine learning \stefania{cite}. Compared to classical deep neural networks dealing with regular grid-structured data, graph neural networks take into account irregular graphs to better learn complex interactions in the data. Although graphs can describe complex systems of relations ranging from biology to social science, they are intrinsically limited to modeling pairwise interactions. 
\stefania{rewrite this paragraph, taken from G and mine article}
The success of topological methods in studying data, and the parallel establishment of \emph{topological data analysis (TDA)} as a field~\cite{edelsbrunner2000topological, zomorodian2005computing} (see also~\cite{carlsson2008,chazal2017,edelsbrunner2010computational,ghrist2008barcodes} for modern introductions and surveys), have confirmed the usefulness of viewing data through a higher-dimensional analog of graphs~\cite{moore2012,patania2017}. Such a higher-dimensional analog is called a \emph{simplicial complex}, a mathematical object whose structure can  describe $n$-fold interactions between points. Their ability to capture hidden patterns in the data has led to various applications from neurobiology~\cite{giusti2015,reimann2017} to material science~\cite{hiraoka2016}.
\stefania{end paragraph}
In this paper we present the \textit{simplicial neural networks}, a novel neural network framework designing local operations that do message passing on simplicial complexes.
Our method, like GNNs, offers an efficient architecture thanks to our formulation of strictly localized filters only involving operations with a sparse matrix. 
Differently from hypergraph neural networks, in SNNs the message passing operator on \emph{simplices} (of a fixed degree) is sensitive to the topological structure of the complex, a highly relevant feature in TDA. 





















