\section{Introduction}

Graph neural networks (GNNs) have recently become popular in machine learning~\cite{defferrard2016convolutional, bronstein2017geometric, wu2020survey}.
\mdeff{To me, being popular, or novel, is not an argument for being interesting or important. I would rather put the emphasis on convolutions as a principled way to exploit the structure which often supports data (for correlation, symmetry, or a substrate for computing / the message-passing view). MLP for ordered feature sets, classic conv for grids/Euclidean, group conv for homogeneous spaces [Cohen et al.], graphs for pairwise, hypergraphs and complexes for $n$-folds.}
% \mdeff{convolutions exploit the symmetries of the space, which (i) reduce learning complexity and (ii) provide a generalization guarantee}
% \mdeff{It's even more than symmetry: works locally if the space doesn't have global symmetries. That might be well formulated in the "Natural NNs" by Cohen et al., I didn't read it yet.}
Compared to classical convolutional deep neural networks that work with regular grid-structured data, graph neural networks can take into account irregular graphs to better learn interactions in the data~\cite{battaglia2018relational}. Although graphs are useful in describing complex systems of irregular relations in a variety of settings, they are intrinsically limited to modeling pairwise relationships. The advance of topological methods in machine learning~\cite{Gabrielsson2020topological, Hofer2019LearningRO, rieck2018neural}, and the earlier establishment of \emph{topological data analysis (TDA)}~\cite{carlsson2008,chazal2017,edelsbrunner2010computational,ghrist2008barcodes} as a field in its own right, have confirmed the usefulness of viewing data as topological spaces in general, or in particular as simplicial complexes, which can be thought of as higher-dimensional analog of graphs~\cite{moore2012,patania2017}.


We here take the view that structure is encoded in \emph{simplicial complexes}, and that these represent $n$-fold interactions. In this setting, we present \emph{simplicial neural networks (SNNs)}, a neural network framework that take into account locality of data living over a simplicial complex in the same way a GNN does for graphs or a conventional CNN does for grids.
\mdeff{I would put the above in 1st paragraph (importance of structure for learning) and remove the below.}
We establish some useful properties of SNNs, indicate how they can be implemented as sparse matrix multiplications, and finally demonstrate their efficacy on a complex built from coauthorship data.

Higher-order relational learning has already proved useful in some applications, e.g., proteins~\cite{ze2020graph}. However these approaches, such as also hypergraph neural networks~\cite{feng2018hypergraphs} of motif-based GNNs~\cite{monti2018motif}, do not have connections to the global topological structure of the simplicial complex, a highly relevant aspect in TDA. \mdeff{why?}
This leads us to believe that our method is far better suited for situations where topological structure is relevant, such as perhaps in the processing of data that exists naturally as vector fields or data that is sensitive to the space's global structure~\cite{deepsphere}.
\mdeff{Geometric / physical applications (vector fields but also fluxes in fluid dynamics or electromagnetism) indeed fit well, and is the source of the continuous theory. But we demonstrate it on purely combinatorial data. Could subset closure be another argument? Complexes seem more natural to me for collaborations because of that.}
