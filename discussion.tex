\section{Conclusion and Future Work}

We introduced a mathematical framework to design neural networks based on a simplicial complex and provided preliminary results on their ability to impute missing data.
We believe that SNNs can provide a suitable analytic method for data that is intrinsically structured to represent $n$-fold interactions.
Our future work will focus on two computational directions:  i) applying the SNNs to vector field data ii) comparing the previous and new results with state of the art imputations algorithms, as in~\cite{spinelli2020neural}.
In parallel, on the theoretical side we will investigate the following two problems: i) generalizing the processes of coarsening and pooling to SNNs. ii) studying the expressive power of SNNs similarly to~\cite{morris2019weisfeiler}. The first point will involve developing an efficient higher dimensional clustering algorithm for coarsening and to find a meaningful rearrangement of the clustered $k$-simplices for an efficient pooling.
\mdeff{I would be less specific.}

Unrelated to the simplicial nature of the work presented here, we would also like to emphasize the way that the spectral language was key to developing and even formulating our method. In the same way that this view has often proven itself highly useful in extracting the important features of certain kinds of data (cf.\ the disciplines of image and audio compression), we believe it is underutilized in machine learning.
\mdeff{I like that. ;)}
Similarly, we wish to extend our method beyond Fourier-based filters, and wonder if there are gains to be had by for example wavelet filters, and whether they are natural in the context of simplicial complexes.
\mdeff{Wavelet filters can be expressed/learned in the current formalism.}
