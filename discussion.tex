\section{Conclusion and future work}

We introduced a mathematical framework to design neural networks for data that live on simplicial complexes and provided preliminary results on their ability to impute missing data.
Future work might include:
(i) comparing SNNs with state-of-the-art imputation algorithms,
(ii) using SNNs to solve vector field problems,
(iii) generalizing coarsening and pooling to simplicial complexes,
(iv) using boundaries and coboundaries to mix data structured by relationships of different dimensions,
and (v) studying the expressive power of SNNs.
% Their expressive power remains to be fully understood on non-homogeneous spaces.

Unrelated to the simplicial nature of this work, we would like to emphasize how the spectral language was key to developing and even formulating our method.
% Fourier analysis was developed to exploit symmetries in solving PDEs, and later used for data analysis and processing.
On homogeneous spaces, convolutions are defined as inner-products with filters shifted by the actions of a symmetry group of the space.
They are the most general shift-invariant linear operators.
% Convolutions are a sufficient and necessary condition for shift-invariance~\cite{kondor2018groupnn}
On non-homogeneous spaces however, the spectral language yields generalized convolutions which are inner-products with \emph{localized} filters~\cite[Sec.~2.4]{perraudin2019deepsphere}. %~\cite[Sec.~2.2]{perraudin2017stationarity}.
Those too are invariant to any symmetry the space might have.
% generality of spaces: homogeneous ⊂ global symmetries ⊂ some automorphisms (local symmetries) ⊂ asymmetric
% (homogeneous == any point is moved to any other by an automorphism)
Convolutions exploit the space's structure to reduce learning complexity by sharing learnable weights through shifts and localizations of filters.
