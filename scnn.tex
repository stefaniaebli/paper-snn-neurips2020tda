%\subsection{Simplicial convolutions}

\textbf{Simplicial Convolution.} Classical CNNs were introduced with image data, or other similarly structured data, in mind. Indeed, they can be seen as a way to exploit the regularity of pixels (or voxels, or toxels, and so forth) and their neighbor relations. \gard{The previous two sentences can be removed if space constraints make it necessary.} This is achieved by a convolutional layer being of the form $f\ast \varphi_W$ where $\varphi_W$ is a function \emph{with small support} parameterized by some learnable weights $W$, and $\ast$ denotes convolution. Typically, $\varphi_W$ just takes constant values determined by $W$ in a small spatial region, and is zero elsewhere. While this seems like an overly complicated way to phrase an idea that is nearly trivial for discrete data on a regular rectangular domain, it does lend itself to a spectral interpretation that we exploit to formulate the CNNs in a much more general setting, as was originally done for graph CNNs.

Motivated by the fact that the discrete Fourier transform of real-valued functions on an $n$-dimensional cubical grid coincides with decomposition of such a function into the eigenfunctions of the graph Laplacian for that grid, we define the Fourier transform of real $p$-cochains on a simplicial complex with Laplacians $\mathcal{L}_p$ as
\begin{align*}
  &\mathcal{F}_p: C^p(K) \to \mathbb{R}^{\lvert K_p \rvert} \\
  &\mathcal{F}_p(c) = \left(\ip{c}{e_1}_p, \ip{c}{e_2}_p, \dotsc, \ip{c}{e_{\lvert K_p \rvert}}_p\right),
\end{align*}
where the $e_i$'s are the eigencochains of $\mathcal{L}_p$ ordered ascendingly by eigenvalues (which we label $\lambda_1\leq\dotsm\leq\lambda_{\lvert K_p \rvert}$). $\mathcal{F}_p$ is invertible since $\mathcal{L}_p$ is diagonalizable; explicitly, if we write $U\diag(\Lambda)U\transpose$ for a normalized eigendecomposition, the orthonormal matrices $U$ and $U\transpose$ represent $\mathcal{F}\inv_p$ and $\mathcal{F}_p$, respectively. This is the foundation for Barbarossa's development of signal processing on simplicial complexes~\cite{barbarossa2018learning}.

Recall that for function classes for which it is defined, the ordinary notions of Fourier transform obey $\mathcal{F}(f\ast g)=\mathcal{F}(f)\mathcal{F}(g)$, where the right hand side denotes pointwise multiplication. This will be our definition of convolution in the simplicial setting. Indeed, for cochains $c,c'\in C^p(K)$ we define their convolution as the cochain
\begin{equation*}
  c\ast_p c' = \mathcal{F}_p\inv\left(\mathcal{F}_p(c)\cdot\mathcal{F}_p(c')\right),
\end{equation*}
where $\cdot$ denotes pointwise multiplication.

Within this framework, we are led to define a \emph{simplicial convolutional layer} with input $p$-cochain $c$ and weights $W$ as being of the form
\begin{equation*}
  \mathcal{F}\inv_p(\varphi_W)\ast_p c  
\end{equation*}
for some as of yet unspecified $\varphi_W\in\mathbb{R}^{\lvert K_p \rvert}$ parameterized by learnable weights $W$. To ensure the central property that a convolutional layer be localizing, we demand that $\varphi_W$ be a low-degree polynomial in $\Lambda=(\lambda_1, \dotsc, \lambda_{\lvert K_p \rvert})$, namely
\begin{equation*}
  \varphi_W = \sum_{i=0}^N W_i\Lambda^i = \sum_{i=0}^N W_i(\lambda^i_1, \lambda^i_2, \dotsc, \lambda^i_{\lvert K_p \rvert})
\end{equation*}
for small $N$. In signal processing parlance, one would say that such a convolutional layer \emph{learns filters that are low-degree polynomials in the frequency domain}.

The reason for restricting the filters to be these low-degree polynomials (in the frequency domain) is best appreciated when writing out the convolutional layer in a basis. Let $L^i_p$ denote the $i$'th power of the matrix for $\mathcal{L}_p$ in, say, the standard basis for $C^p(K)$, and similarly for $c$. Then
\begin{equation}
  \mathcal{F}\inv_p(\varphi_W)\ast_p c = \sum_{i=0}^N W_iU\diag(\Lambda^i)U\transpose c = \sum_{i=0}^N W_i\left(U\diag(\Lambda)U\transpose\right)^i c = \sum_{i=0}^NW_iL^i_pc. \label{eq:filter}
\end{equation}
This is important for two reasons. First, it shows that the layer can be efficiently implemented by $N$ sparse matrix-vector multiplications (or $\mathcal{O}(N^2)$ of them if the most sparsity is desired). Like for traditional CNN layers, this reduces the computational complexity of the layer from $\mathcal{O}(\lvert K_p\rvert^2)$ to $\mathcal{O}(\xi\lvert K_p\rvert)$ when the $\xi$ is the density factor. Second, it shows that the operation is $N$-localizing in the following sense. Suppose that $\sigma$ and $\tau$ are $p$-simplices for which $(\nu_0, \nu_1, \dotsc, \nu_d)$ is the shortest sequence of $p$-simplices with the property that $\nu_0=\sigma$, $\nu_d=\tau$, and each $\nu_i$ shares a face or a coface with $\nu_{i-1}$, and a face or a coface with $\nu_{i+1}$. We say that $d$ is the \emph{simplicial distance} between $\sigma$ and $\tau$. Then for all $N<d$, the entry of $L_p^N$ corresponding to $\sigma$ and $\tau$ is $0$, and so the filter does not cause interaction between $c(\sigma)$ and $c(\tau)$. This is analogous to a size-$d$ ordinary CNN layer not distributing information between pixels that are more than $d$ pixels apart. We will refer to $N$ as the \emph{degree} of the convolutional layer, but one may well wish to keep in mind the notion of \emph{size} from traditional CNNs.

In practice we implement \refeq{filter} using Chebyshev polynomials, as suggested for GNNs in~\cite{defferrard2016convolutional}.

\stefania{Kathryn: What do you think about writing since the beginning all the operation using $U$? Guillaume was suggetsing that it might be easier to read in that way. However as it is written now I think it is more elegant.   }

\gard{Re Stefania's comment above: I understand the point, but I don't think it becomes easier to read. It becomes more like Barbarossa's and other EE papers, perhaps, but that just mires the reader in keeping track of seemingly arbitrary bases (beyond the arbitrary ordering of eigencochains). With the above formulation, one can trace a path all the way back to the most basic notion of a CNN if one interprets the first appearance of ``$f\ast \varphi_W$'' the right way. I really think that that way of understanding even the classical CNNs is the best chance one has to make them less ad-hoc and more generalizable. While I don't know if there are practical implications of this, it does keep an open road directly to ``deep learning over locally compact groups'', for example. As far as I know, people haven't gone there yet, but it doesn't seem right not to leave the road open (not that we discovered the road --- the road is obvious.)}
